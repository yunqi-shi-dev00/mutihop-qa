"""
åŠå¯¼ä½“QAç”Ÿæˆç³»ç»Ÿ - çŸ¥è¯†åº“æ¨¡å—ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
åŒ…å«å¢å¼ºçš„çŸ¥è¯†åº“ã€QAå®ä½“ç±»å’ŒAgentè®°å¿†ç±»
æ–°å¢åŠŸèƒ½ï¼šè¯­ä¹‰embeddingæ”¯æŒï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
"""

import random
from collections import defaultdict
from typing import Dict, List, Any, Optional

# å¯é€‰ä¾èµ–ï¼šè¯­ä¹‰embedding
try:
    import numpy as np
    from sklearn.metrics.pairwise import cosine_similarity
    import torch
    from transformers import AutoTokenizer, AutoModel
    EMBEDDING_AVAILABLE = True
    print("[INFO] âœ“ Embeddingä¾èµ–å·²åŠ è½½ï¼ˆä½¿ç”¨æœ¬åœ°Qwen3-Embeddingæ¨¡å‹ï¼‰")
except ImportError as e:
    EMBEDDING_AVAILABLE = False
    print(f"[WARNING] Embeddingä¾èµ–å¯¼å…¥å¤±è´¥: {e}")
    print("[WARNING] å°†ä½¿ç”¨å…³é”®è¯åŒ¹é…æ¨¡å¼")
except Exception as e:
    EMBEDDING_AVAILABLE = False
    print(f"[ERROR] åŠ è½½embeddingä¾èµ–æ—¶å‡ºé”™: {e}")
    print("[WARNING] å°†ä½¿ç”¨å…³é”®è¯åŒ¹é…æ¨¡å¼")


class EnhancedSemiconductorKB:
    """å¢å¼ºç‰ˆçŸ¥è¯†åº“ - åŸç‰ˆåŠŸèƒ½ + æ¶ˆè€—è¿½è¸ª + åŠ¨æ€è§„åˆ’ + è¯­ä¹‰embedding"""
    
    def __init__(self, qa_data: List[Dict], use_embedding: bool = False, embedding_batch_size: int = 4, embedding_model_path: str = None):
        """
        Args:
            qa_data: QAæ•°æ®åˆ—è¡¨
            use_embedding: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰embeddingæŸ¥æ‰¾ç›¸å…³QAï¼ˆä½¿ç”¨æœ¬åœ°Qwen3-Embeddingæ¨¡å‹ï¼‰
            embedding_batch_size: Embeddingç”Ÿæˆçš„æ‰¹é‡å¤§å°ï¼ˆé»˜è®¤4ï¼Œå‡å°‘å†…å­˜å ç”¨ï¼‰
            embedding_model_path: Embeddingæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨Qwen3-Embedding-0.6Bï¼‰
        """
        # ========================================
        # ğŸ”§ ä¿®å¤Bug 7ï¼šEmbeddingæ¨¡å‹å†…å­˜ä¸è¶³
        # ä¿®å¤æ—¶é—´ï¼š2025-11-19
        # é—®é¢˜ï¼šbatch_sizeç¡¬ç¼–ç ä¸º8ï¼Œå¯¼è‡´GPUå†…å­˜ä¸è¶³ï¼ˆOOMï¼‰
        # è§£å†³ï¼šæ”¯æŒè‡ªå®šä¹‰batch_sizeï¼Œé»˜è®¤æ”¹ä¸º4ï¼Œç”¨æˆ·å¯é€šè¿‡å‚æ•°è°ƒæ•´
        # ä½¿ç”¨ï¼š--embedding-batch-size 2ï¼ˆ3-4GBæ˜¾å­˜ï¼‰æˆ– 1ï¼ˆ2.5-3GBæ˜¾å­˜ï¼‰
        # ========================================
        # â­â­â­ ä¼˜åŒ–ï¼šæ”¯æŒè‡ªå®šä¹‰embedding batch_size â­â­â­
        self.embedding_batch_size = embedding_batch_size
        self.embedding_model_path = embedding_model_path  # â­ æ–°å¢ï¼šè‡ªå®šä¹‰æ¨¡å‹è·¯å¾„
        self.qa_data = {qa['id']: qa for qa in qa_data}
        self.qa_ids = list(self.qa_data.keys())
        
        # âœ… åŸç‰ˆç´¢å¼•ï¼ˆå®Œå…¨ä¿ç•™ï¼‰
        self.concept_to_qas = defaultdict(list)
        self.qa_to_concepts = defaultdict(list)
        self.paper_to_qas = defaultdict(list)
        self.qa_to_paper = {}
        
        # ğŸ†• æ–°å¢ï¼šæ¶ˆè€—è¿½è¸ªç³»ç»Ÿ
        self.paper_usage = defaultdict(int)
        self.paper_total_qa = defaultdict(int)
        self.paper_usage_rate = {}
        self.completed_papers = set()
        self.active_papers = set()
        self.paper_quality_score = defaultdict(float)
        
        # ğŸš€ æ–°å¢ï¼šè¯­ä¹‰embeddingç³»ç»Ÿ
        self.use_embedding = use_embedding and EMBEDDING_AVAILABLE
        self.embedding_model = None
        self.embedding_tokenizer = None
        self.qa_embeddings = None
        self.qa_id_to_idx = {}  # QA-IDåˆ°ç´¢å¼•çš„æ˜ å°„
        
        self._build_indexes()
        self._initialize_usage_tracking()
        
        # æ„å»ºembedding
        if self.use_embedding:
            print("[KB] ğŸš€ å¯ç”¨è¯­ä¹‰embeddingæ¨¡å¼ï¼ˆæœ¬åœ°Qwen3-Embeddingæ¨¡å‹ï¼‰")
            self._build_embeddings()
        else:
            if use_embedding and not EMBEDDING_AVAILABLE:
                print("[KB] âš ï¸ æœªå®‰è£…embeddingä¾èµ–ï¼Œä½¿ç”¨å…³é”®è¯åŒ¹é…æ¨¡å¼")
            else:
                print("[KB] ä½¿ç”¨å…³é”®è¯åŒ¹é…æ¨¡å¼")
        
        print(f"[KB] åŠ è½½ {len(self.qa_data)} æ¡QAæ•°æ®")
        print(f"[KB] è®ºæ–‡æ•°é‡: {len(self.paper_to_qas)}")
        print(f"[KB] æ´»è·ƒè®ºæ–‡: {len(self.active_papers)}")
    
    def _build_indexes(self):
        """æ„å»ºç´¢å¼•ï¼ˆåŸç‰ˆé€»è¾‘ï¼‰"""
        for qa_id, qa in self.qa_data.items():
            paper = qa.get('paper_name', 'unknown')
            self.paper_to_qas[paper].append(qa_id)
            self.qa_to_paper[qa_id] = paper
            
            concepts = self._extract_concepts_simple(qa['question'] + ' ' + qa['answer'])
            for concept in concepts:
                self.concept_to_qas[concept].append(qa_id)
                self.qa_to_concepts[qa_id].append(concept)
    
    def _initialize_usage_tracking(self):
        """ğŸ†• åˆå§‹åŒ–æ¶ˆè€—è¿½è¸ª"""
        for paper_name, qa_list in self.paper_to_qas.items():
            self.paper_total_qa[paper_name] = len(qa_list)
            self.paper_usage[paper_name] = 0
            self.paper_usage_rate[paper_name] = 0.0
            self.active_papers.add(paper_name)
            self.paper_quality_score[paper_name] = 1.0
    
    def _build_embeddings(self):
        """ğŸš€ æ„å»ºQAçš„embeddingå‘é‡ï¼ˆä½¿ç”¨æœ¬åœ°Qwen3-Embeddingæ¨¡å‹ï¼‰"""
        try:
            # ========================================
            # ğŸ”§ ä¼˜åŒ–10ï¼šæ”¯æŒè‡ªå®šä¹‰embeddingæ¨¡å‹è·¯å¾„
            # é—®é¢˜ï¼šç”¨æˆ·å¯èƒ½ç”¨é”™æ¨¡å‹ï¼ˆå¦‚7Bæ¨¡å‹ï¼‰ï¼Œå¯¼è‡´é€Ÿåº¦æ…¢
            # è§£å†³ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ¨¡å‹è·¯å¾„
            # ========================================
            # ä½¿ç”¨æœ¬åœ°Qwen3-Embeddingæ¨¡å‹ï¼ˆé»˜è®¤0.6Bï¼Œå¿«é€Ÿï¼‰
            if self.embedding_model_path:
                local_model_path = self.embedding_model_path
                print(f"[KB] ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„embeddingæ¨¡å‹: {local_model_path}")
            else:
                local_model_path = "/mnt/data/LLM/hhh/qwen3_emb/backup_h/Qwen3-Embedding-0.6B_sft_v5"
                print(f"[KB] ä½¿ç”¨é»˜è®¤embeddingæ¨¡å‹: {local_model_path}")
            # ========================================
            
            print(f"[KB] åŠ è½½æœ¬åœ°embeddingæ¨¡å‹: {local_model_path}")
            
            self.embedding_tokenizer = AutoTokenizer.from_pretrained(
                local_model_path, 
                trust_remote_code=True
            )
            self.embedding_model = AutoModel.from_pretrained(
                local_model_path, 
                trust_remote_code=True
            )
            self.embedding_model.eval()
            
            # ========================================
            # ğŸ”§ ä¼˜åŒ–10ï¼šæ™ºèƒ½è®¾å¤‡é€‰æ‹©ï¼ˆä¼˜å…ˆGPUï¼ŒCPUä½œä¸ºfallbackï¼‰
            # é—®é¢˜ï¼šå¼ºåˆ¶CPUå¯¼è‡´7Bæ¨¡å‹å¤ªæ…¢
            # è§£å†³ï¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œå¦‚æœGPUä¸å¯ç”¨æˆ–æ˜¾å­˜ä¸è¶³åˆ™ä½¿ç”¨CPU
            # ========================================
            # æ£€æµ‹è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼‰
            if torch.cuda.is_available():
                device = "cuda"
                print(f"[KB] æ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨è®¾å¤‡: cuda")
            else:
                device = "cpu"
                print(f"[KB] æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨è®¾å¤‡: cpu")
            
            try:
                self.embedding_model = self.embedding_model.to(device)
                print(f"[KB] âœ“ æ¨¡å‹å·²åŠ è½½åˆ°: {device}")
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print(f"[KB] âš ï¸ GPUæ˜¾å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPU")
                    device = "cpu"
                    self.embedding_model = self.embedding_model.to(device)
                else:
                    raise
            # ========================================
            
            # å‡†å¤‡QAæ–‡æœ¬
            qa_texts = []
            qa_ids = []
            
            for qa_id, qa in self.qa_data.items():
                text = qa.get('question', '') + ' ' + qa.get('answer', '')
                qa_texts.append(text)
                qa_ids.append(qa_id)
            
            print(f"[KB] ç”Ÿæˆ {len(qa_texts)} ä¸ªQAçš„embeddingå‘é‡...")
            
            # æ‰¹é‡ç”Ÿæˆembedding
            embeddings_list = []
            # ========================================
            # ğŸ”§ ä¼˜åŒ–10ï¼šæ™ºèƒ½batch_sizeï¼ˆGPUæ—¶è‡ªåŠ¨å¢å¤§ï¼‰
            # é—®é¢˜ï¼šå›ºå®šbatch_size=4å¯¹GPUæ¥è¯´å¤ªå°ï¼Œé€Ÿåº¦æ…¢
            # è§£å†³ï¼šGPUæ—¶é»˜è®¤ä½¿ç”¨æ›´å¤§çš„batch_sizeï¼ˆå¦‚32ï¼‰ï¼ŒCPUæ—¶ä½¿ç”¨å°batch
            # ========================================
            # â­â­â­ ä¼˜åŒ–ï¼šæ™ºèƒ½batch_size â­â­â­
            if hasattr(self, 'embedding_batch_size') and self.embedding_batch_size > 0:
                # ç”¨æˆ·æŒ‡å®šäº†batch_sizeï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„
                batch_size = self.embedding_batch_size
            else:
                # è‡ªåŠ¨é€‰æ‹©batch_size
                if device == "cuda":
                    batch_size = 32  # GPUé»˜è®¤32ï¼ˆå¿«é€Ÿï¼‰
                else:
                    batch_size = 4   # CPUé»˜è®¤4ï¼ˆé¿å…æ…¢ï¼‰
            print(f"[KB] Embedding batch_size: {batch_size} (è®¾å¤‡: {device})")
            # ========================================
            
            with torch.no_grad():
                for i in range(0, len(qa_texts), batch_size):
                    batch_texts = qa_texts[i:i+batch_size]
                    
                    # Tokenize
                    inputs = self.embedding_tokenizer(
                        batch_texts, 
                        padding=True, 
                        truncation=True, 
                        max_length=512,
                        return_tensors="pt"
                    ).to(device)
                    
                    # è·å–embedding
                    outputs = self.embedding_model(**inputs)
                    
                    # Mean pooling
                    attention_mask = inputs['attention_mask']
                    token_embeddings = outputs.last_hidden_state
                    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
                    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
                    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
                    batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()
                    
                    embeddings_list.append(batch_embeddings)
                    
                    # ========================================
                    # ğŸ”§ ä¿®å¤Bug 7ï¼šåŠæ—¶æ¸…ç†æ˜¾å­˜
                    # è¯´æ˜ï¼šæ¯ä¸ªbatchåç«‹å³åˆ é™¤ä¸­é—´å˜é‡å¹¶æ¸…ç†GPUç¼“å­˜
                    # æ•ˆæœï¼šå‡å°‘å³°å€¼å†…å­˜å ç”¨ï¼Œé¿å…OOM
                    # ========================================
                    # â­â­â­ ä¼˜åŒ–ï¼šåŠæ—¶æ¸…ç†æ˜¾å­˜ â­â­â­
                    del inputs, outputs, token_embeddings, input_mask_expanded
                    if device == "cuda":
                        torch.cuda.empty_cache()
                    # ========================================
                    
                    # ========================================
                    # ğŸ”§ ä¼˜åŒ–10ï¼šæ”¹è¿›è¿›åº¦æ˜¾ç¤º
                    # æ˜¾ç¤ºè¿›åº¦ç™¾åˆ†æ¯”å’Œé¢„ä¼°æ—¶é—´
                    # ========================================
                    progress = min(i + batch_size, len(qa_texts))
                    percent = progress * 100.0 / len(qa_texts)
                    print(f"   è¿›åº¦: {progress}/{len(qa_texts)} ({percent:.1f}%)", end='\r')
                    # ========================================
            
            print()  # æ¢è¡Œ
            
            # åˆå¹¶æ‰€æœ‰batch
            self.qa_embeddings = np.vstack(embeddings_list)
            
            # å½’ä¸€åŒ–ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
            norms = np.linalg.norm(self.qa_embeddings, axis=1, keepdims=True)
            self.qa_embeddings = self.qa_embeddings / norms
            
            # å»ºç«‹IDåˆ°ç´¢å¼•çš„æ˜ å°„
            for idx, qa_id in enumerate(qa_ids):
                self.qa_id_to_idx[qa_id] = idx
            
            print(f"[KB] âœ“ Embeddingæ„å»ºå®Œæˆï¼ˆç»´åº¦: {self.qa_embeddings.shape[1]}ï¼‰")
            
            # æ¸…ç†æ¨¡å‹é‡Šæ”¾å†…å­˜
            del self.embedding_model
            del self.embedding_tokenizer
            self.embedding_model = None
            self.embedding_tokenizer = None
            
            if device == "cuda":
                torch.cuda.empty_cache()
            
            print(f"[KB] âœ“ æ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
            
        except Exception as e:
            print(f"[KB] âœ— Embeddingæ„å»ºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print(f"[KB] å›é€€åˆ°å…³é”®è¯åŒ¹é…æ¨¡å¼")
            self.use_embedding = False
            self.embedding_model = None
            self.embedding_tokenizer = None
            self.qa_embeddings = None
    
    def _extract_concepts_simple(self, text: str) -> List[str]:
        """ç®€å•æ¦‚å¿µæå–ï¼ˆåŸç‰ˆé€»è¾‘ï¼‰"""
        keywords = [
            'æ°§åŒ–ç‰©', 'è–„è†œæ™¶ä½“ç®¡', 'TFT', 'è½½æµå­', 'è¿ç§»ç‡', 'é˜ˆå€¼ç”µå‹',
            'æ°§ç©ºä½', 'æ …æ', 'æºæ', 'æ¼æ', 'æ²Ÿé“', 'ä»‹ç”µå±‚',
            'IGZO', 'LTPS', 'a-Si', 'OLED', 'LCD',
            'æº…å°„', 'é€€ç«', 'åˆ»èš€', 'æ²‰ç§¯', 'é’åŒ–',
            'ç”µå­', 'ç©ºç©´', 'èƒ½å¸¦', 'è´¹ç±³èƒ½çº§', 'æ€å¯†åº¦',
            'åŠå¯¼ä½“', 'æ™¶ä½“ç®¡', 'å™¨ä»¶', 'ææ–™', 'å·¥è‰º'
        ]
        
        concepts = []
        for keyword in keywords:
            if keyword in text:
                concepts.append(keyword)
        return list(set(concepts))
    
    # âœ… åŸç‰ˆæ–¹æ³•ï¼šfind_related_qasï¼ˆå®Œå…¨ä¿ç•™ï¼‰
    def find_related_qas(self, qa_id: str, top_k: int = 5) -> List[str]:
        """æ‰¾åˆ°ç›¸å…³çš„QAï¼ˆåŸç‰ˆé€»è¾‘ï¼ŒåŸºäºå…³é”®è¯åŒ¹é…ï¼‰"""
        if qa_id not in self.qa_to_concepts:
            return random.sample(self.qa_ids, min(top_k, len(self.qa_ids)))
        
        concepts = self.qa_to_concepts[qa_id]
        scores = defaultdict(int)
        
        for concept in concepts:
            for related_qa_id in self.concept_to_qas[concept]:
                if related_qa_id != qa_id:
                    scores[related_qa_id] += 1
        
        sorted_qas = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        related_ids = [qa_id for qa_id, _ in sorted_qas[:top_k]]
        
        if len(related_ids) < top_k:
            remaining = [qid for qid in self.qa_ids if qid != qa_id and qid not in related_ids]
            related_ids.extend(random.sample(remaining, min(top_k - len(related_ids), len(remaining))))
        
        return related_ids
    
    # ğŸ†• æ–°å¢ï¼šåŸºäºä¼˜å…ˆçº§çš„ç›¸å…³QAæŸ¥æ‰¾ï¼ˆæ”¯æŒembeddingå’Œå…³é”®è¯ä¸¤ç§æ¨¡å¼ï¼‰
    def find_related_qas_prioritized(self, qa_id: str, top_k: int = 5, current_stage: str = 'early') -> List[str]:
        """æ‰¾åˆ°ç›¸å…³çš„QAï¼ˆå¸¦ä¼˜å…ˆçº§ï¼Œæ”¯æŒembeddingï¼‰"""
        
        if self.use_embedding and self.qa_embeddings is not None:
            # ä½¿ç”¨è¯­ä¹‰embedding
            return self._find_related_by_embedding(qa_id, top_k, current_stage)
        else:
            # ä½¿ç”¨åŸæœ‰çš„å…³é”®è¯åŒ¹é…
            return self._find_related_by_keywords(qa_id, top_k, current_stage)
    
    def _find_related_by_embedding(self, qa_id: str, top_k: int, current_stage: str) -> List[str]:
        """ğŸš€ åŸºäºè¯­ä¹‰embeddingæŸ¥æ‰¾ç›¸å…³QA"""
        if qa_id not in self.qa_id_to_idx:
            return self._find_related_by_keywords(qa_id, top_k, current_stage)
        
        qa_idx = self.qa_id_to_idx[qa_id]
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå·²å½’ä¸€åŒ–ï¼Œç›´æ¥ç‚¹ç§¯å³å¯ï¼‰
        query_embedding = self.qa_embeddings[qa_idx].reshape(1, -1)
        similarities = cosine_similarity(query_embedding, self.qa_embeddings)[0]
        
        # æ’åºï¼ˆæ’é™¤è‡ªå·±ï¼‰
        similar_indices = np.argsort(similarities)[::-1]
        
        results = []
        for idx in similar_indices:
            if idx != qa_idx:
                # æ ¹æ®ç´¢å¼•æ‰¾åˆ°QA-ID
                target_qa_id = None
                for qid, qidx in self.qa_id_to_idx.items():
                    if qidx == idx:
                        target_qa_id = qid
                        break
                
                if target_qa_id and target_qa_id in self.qa_data:
                    similarity_score = float(similarities[idx])
                    
                    # åŠ ä¸Šè®ºæ–‡ä¼˜å…ˆçº§æƒé‡
                    paper = self.qa_to_paper.get(target_qa_id, 'unknown')
                    priority = self.get_paper_priority(paper)
                    
                    # ç»¼åˆå¾—åˆ†ï¼šè¯­ä¹‰ç›¸ä¼¼åº¦ (0-1) + è®ºæ–‡ä¼˜å…ˆçº§ (0-1)
                    combined_score = similarity_score * 0.7 + (priority / 10.0) * 0.3
                    
                    results.append({
                        'qa_id': target_qa_id,
                        'score': combined_score,
                        'similarity': similarity_score,
                        'priority': priority
                    })
            
            if len(results) >= top_k * 2:  # è·å–2å€å€™é€‰ï¼Œç”¨äºåŠ¨æ€è§„åˆ’
                break
        
        # åº”ç”¨åŠ¨æ€è§„åˆ’è°ƒæ•´
        adjusted_results = self._apply_dynamic_planning(results, current_stage)
        
        final_results = [r['qa_id'] if isinstance(r, dict) else r for r in adjusted_results[:top_k]]
        
        # ========================================
        # ğŸ”§ ä¿®å¤Bug 6ï¼šç›¸å…³QAå®ä½“åˆ—è¡¨åªæœ‰1ä¸ª
        # ä¿®å¤æ—¶é—´ï¼š2025-11-19
        # é—®é¢˜ï¼šembeddingæŸ¥æ‰¾å¯èƒ½è¿”å›å¾ˆå°‘çš„ç›¸å…³QAï¼ˆå¦‚1ä¸ªï¼‰ï¼Œå¯¼è‡´æ— æ³•æ¡¥è”
        # è§£å†³ï¼šä¿åº•æœºåˆ¶ï¼Œè‡ªåŠ¨è¡¥å……éšæœºQAï¼Œç¡®ä¿è‡³å°‘è¿”å›top_kä¸ª
        # ========================================
        # â­â­â­ ä¿åº•æœºåˆ¶ï¼šå¦‚æœç»“æœå¤ªå°‘ï¼Œè¡¥å……éšæœºQA â­â­â­
        if len(final_results) < top_k:
            remaining_qas = [qid for qid in self.qa_ids if qid != qa_id and qid not in final_results]
            if remaining_qas:
                additional_count = min(top_k - len(final_results), len(remaining_qas))
                sampled = random.sample(remaining_qas, additional_count)
                final_results.extend(sampled)
                print(f"[KB] ä¿åº•è¡¥å……ï¼šembeddingæ‰¾åˆ°{len(final_results)-len(sampled)}ä¸ªï¼Œè¡¥å……{len(sampled)}ä¸ªï¼Œæ€»è®¡{len(final_results)}ä¸ª")
            else:
                print(f"[KB] âš ï¸ æ— å¯è¡¥å……çš„QAï¼ˆKBæ€»æ•°: {len(self.qa_ids)}ï¼‰")
        # ========================================
        
        return final_results
    
    def _find_related_by_keywords(self, qa_id: str, top_k: int, current_stage: str) -> List[str]:
        """åŸºäºå…³é”®è¯åŒ¹é…æŸ¥æ‰¾ç›¸å…³QAï¼ˆåŸæœ‰é€»è¾‘å¢å¼ºç‰ˆï¼‰"""
        if qa_id not in self.qa_to_concepts:
            # æŒ‰è®ºæ–‡ä¼˜å…ˆçº§æ’åº
            active_papers = list(self.active_papers)
            active_papers.sort(key=lambda p: self.get_paper_priority(p), reverse=True)
            
            candidates = []
            for paper in active_papers:
                candidates.extend(self.paper_to_qas[paper])
                if len(candidates) >= top_k:
                    break
            
            return random.sample(candidates, min(top_k, len(candidates)))
        
        concepts = self.qa_to_concepts[qa_id]
        scores = defaultdict(float)
        
        for concept in concepts:
            for related_qa_id in self.concept_to_qas[concept]:
                if related_qa_id != qa_id:
                    scores[related_qa_id] += 1.0
                    
                    # åŠ ä¸Šè®ºæ–‡ä¼˜å…ˆçº§æƒé‡
                    paper = self.qa_to_paper.get(related_qa_id, 'unknown')
                    priority = self.get_paper_priority(paper)
                    scores[related_qa_id] += priority / 10.0
        
        sorted_qas = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        results = [{'qa_id': qa_id, 'score': score} for qa_id, score in sorted_qas]
        
        # åº”ç”¨åŠ¨æ€è§„åˆ’è°ƒæ•´
        adjusted_results = self._apply_dynamic_planning(results, current_stage)
        
        related_ids = [r['qa_id'] if isinstance(r, dict) else r for r in adjusted_results[:top_k]]
        
        if len(related_ids) < top_k:
            remaining = [qid for qid in self.qa_ids if qid != qa_id and qid not in related_ids]
            related_ids.extend(random.sample(remaining, min(top_k - len(related_ids), len(remaining))))
        
        return related_ids
    
    def _apply_dynamic_planning(self, results: List[Dict], current_stage: str) -> List[Dict]:
        """åº”ç”¨åŠ¨æ€è§„åˆ’ç­–ç•¥è°ƒæ•´ä¼˜å…ˆçº§"""
        if current_stage == 'early':
            # æ—©æœŸï¼šéšæœºæ‰“ä¹±ï¼Œé¼“åŠ±æ¢ç´¢
            random.shuffle(results)
        elif current_stage == 'mid':
            # ä¸­æœŸï¼š70%æŒ‰å¾—åˆ†ï¼Œ30%æŒ‰ä½¿ç”¨ç‡
            by_score = sorted(results, key=lambda x: x['score'], reverse=True)
            by_usage = sorted(results, key=lambda x: self.paper_usage_rate.get(
                self.qa_to_paper.get(x['qa_id'], 'unknown'), 0.0
            ))
            
            split = int(len(results) * 0.7)
            results = by_score[:split] + by_usage[:(len(results) - split)]
        else:
            # åæœŸï¼šä¼˜å…ˆä½ä½¿ç”¨ç‡
            results = sorted(results, key=lambda x: self.paper_usage_rate.get(
                self.qa_to_paper.get(x['qa_id'], 'unknown'), 0.0
            ))
        
        return results
    
    # ğŸ†• æ–°å¢ï¼šè®ºæ–‡ä¼˜å…ˆçº§è®¡ç®—
    def get_paper_priority(self, paper_name: str) -> float:
        """è®¡ç®—è®ºæ–‡çš„é€‰æ‹©ä¼˜å…ˆçº§"""
        if paper_name in self.completed_papers:
            return 0.0
        
        usage_rate = self.paper_usage_rate.get(paper_name, 0.0)
        quality = self.paper_quality_score.get(paper_name, 1.0)
        
        # ä½¿ç”¨ç‡è¶Šä½ï¼Œä¼˜å…ˆçº§è¶Šé«˜
        priority = (1.0 - usage_rate) * 10.0
        priority *= quality
        
        return priority
    
    # ğŸ†• æ–°å¢ï¼šé€‰æ‹©ä½ä½¿ç”¨ç‡QA
    def select_underutilized_qa(self) -> str:
        """é€‰æ‹©ä½ä½¿ç”¨ç‡çš„è®ºæ–‡ä¸­çš„QA"""
        # è·å–ä½¿ç”¨ç‡æœ€ä½çš„è®ºæ–‡
        low_usage_papers = [
            (paper, rate) for paper, rate in self.paper_usage_rate.items()
            if paper in self.active_papers
        ]
        
        if not low_usage_papers:
            return random.choice(self.qa_ids)
        
        low_usage_papers.sort(key=lambda x: x[1])
        selected_paper = low_usage_papers[0][0]
        
        return random.choice(self.paper_to_qas[selected_paper])
    
    # ğŸ†• æ–°å¢ï¼šæ›´æ–°ä½¿ç”¨ç»Ÿè®¡
    def update_usage(self, qa_ids: List[str]):
        """æ›´æ–°ä½¿ç”¨ç»Ÿè®¡"""
        for qa_id in qa_ids:
            paper_name = self.qa_to_paper.get(qa_id, 'unknown')
            if paper_name == 'unknown':
                continue
            
            self.paper_usage[paper_name] += 1
            total = self.paper_total_qa[paper_name]
            usage_rate = self.paper_usage[paper_name] / total
            self.paper_usage_rate[paper_name] = usage_rate
            
            # è‡ªåŠ¨æ ‡è®°å®Œæˆ
            if usage_rate >= 0.8 and paper_name not in self.completed_papers:
                self.completed_papers.add(paper_name)
                self.active_papers.discard(paper_name)
                print(f"[KB] âœ“ è®ºæ–‡å®Œæˆ: {paper_name} (ä½¿ç”¨ç‡: {usage_rate*100:.1f}%)")
    
    # ğŸ†• æ–°å¢ï¼šè·å–æ¶ˆè€—ç»Ÿè®¡
    def get_usage_stats(self) -> Dict:
        """è·å–æ¶ˆè€—ç»Ÿè®¡"""
        total_papers = len(self.paper_to_qas)
        active_papers = len(self.active_papers)
        completed_papers = len(self.completed_papers)
        
        overall_coverage = completed_papers / total_papers if total_papers > 0 else 0.0
        
        low_usage_papers = [
            (paper, rate) for paper, rate in self.paper_usage_rate.items()
            if rate < 0.3 and paper in self.active_papers
        ]
        low_usage_papers.sort(key=lambda x: x[1])
        
        return {
            'total_papers': total_papers,
            'active_papers': active_papers,
            'completed_papers': completed_papers,
            'overall_coverage': overall_coverage,
            'low_usage_papers': low_usage_papers[:10],
            'usage_distribution': {
                '0-20%': sum(1 for r in self.paper_usage_rate.values() if r < 0.2),
                '20-40%': sum(1 for r in self.paper_usage_rate.values() if 0.2 <= r < 0.4),
                '40-60%': sum(1 for r in self.paper_usage_rate.values() if 0.4 <= r < 0.6),
                '60-80%': sum(1 for r in self.paper_usage_rate.values() if 0.6 <= r < 0.8),
                '80-100%': sum(1 for r in self.paper_usage_rate.values() if r >= 0.8)
            }
        }
    
    # âœ… åŸç‰ˆæ–¹æ³•ï¼šget_qaï¼ˆå®Œå…¨ä¿ç•™ï¼‰
    def get_qa(self, qa_id: str) -> Dict:
        return self.qa_data.get(qa_id)
    
    # âœ… åŸç‰ˆæ–¹æ³•ï¼šget_qa_reprï¼ˆå®Œå…¨ä¿ç•™ï¼‰
    def get_qa_repr(self, qa_id: str) -> str:
        qa = self.get_qa(qa_id)
        if not qa:
            return ""
        
        concepts = self.qa_to_concepts.get(qa_id, [])
        concept_str = ', '.join(concepts) if concepts else 'æ— '
        
        return f"""ID: {qa_id}
é—®é¢˜: {qa['question']}
ç­”æ¡ˆ: {qa['answer']}
æ¥æºè®ºæ–‡: {qa.get('paper_name', 'unknown')}
å…³é”®æ¦‚å¿µ: {concept_str}
"""


class SemiconductorQAEntity:
    """åŠå¯¼ä½“QAå®ä½“ï¼ˆåŸç‰ˆå®Œæ•´ä¿ç•™ + ä¿®å¤key_conceptså­—å…¸é—®é¢˜ï¼‰"""
    
    def __init__(self, qa_id: str, qa_data: Dict, kb: EnhancedSemiconductorKB):
        self.id = qa_id
        self.qa_data = qa_data
        self.kb = kb
        self.summary = None
        self.key_concepts = []
        self.related_qas = []
    
    @property
    def name(self):
        return f"QA-{self.id}"
    
    @property
    def url(self):
        return self.id
    
    def repr(self):
        """ç”Ÿæˆå®ä½“çš„æ–‡æœ¬è¡¨ç¤ºï¼ˆâ­ ä¿®å¤ï¼šå…¼å®¹å­—å…¸å’Œå­—ç¬¦ä¸²ä¸¤ç§æ ¼å¼ï¼‰"""
        # â­â­â­ ä¿®å¤ key_concepts å­—å…¸é—®é¢˜ â­â­â­
        if self.key_concepts:
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—ç¬¦ä¸²ï¼ˆæ—§æ ¼å¼ï¼‰
            if isinstance(self.key_concepts[0], str):
                concepts_str = ', '.join(self.key_concepts)
            # å¦‚æœç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å­—å…¸ï¼ˆæ–°æ ¼å¼ï¼š{"name": "...", "type": "..."}ï¼‰
            elif isinstance(self.key_concepts[0], dict):
                concepts_str = ', '.join([c.get('name', str(c)) for c in self.key_concepts])
            else:
                # å…¶ä»–æƒ…å†µï¼Œè½¬ä¸ºå­—ç¬¦ä¸²
                concepts_str = ', '.join([str(c) for c in self.key_concepts])
        else:
            concepts_str = 'å¾…æå–'
            
        related_str = ', '.join([f"QA-{rid}" for rid in self.related_qas[:3]]) if self.related_qas else 'æ— '
        
        question = self.qa_data.get('question', '')
        answer = self.qa_data.get('answer', '')
        paper_name = self.qa_data.get('paper_name', 'unknown')
        
        return f"""# QAå®ä½“ {self.id}

## é—®é¢˜
{question}

## ç­”æ¡ˆ
{answer}

## æ¥æº
è®ºæ–‡: {paper_name}

## å…³é”®æ¦‚å¿µ
{concepts_str}

## ç›¸å…³QA
{related_str}

## æ‘˜è¦
{self.summary or 'å¾…ç”Ÿæˆ'}
"""
    
    def dict(self):
        """å¯¼å‡ºä¸ºå­—å…¸ï¼ˆâ­ ä¿®å¤ï¼šç¡®ä¿ key_concepts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ï¼‰"""
        # â­â­â­ ä¿®å¤ï¼šç¡®ä¿ key_concepts æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ â­â­â­
        if self.key_concepts and len(self.key_concepts) > 0:
            # å¦‚æœæ˜¯å­—å…¸åˆ—è¡¨ï¼Œæå–nameå­—æ®µ
            if isinstance(self.key_concepts[0], dict):
                key_concepts_str = [c.get('name', str(c)) for c in self.key_concepts]
            else:
                # å·²ç»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                key_concepts_str = self.key_concepts
        else:
            key_concepts_str = []
        
        return {
            'id': self.id,
            'name': self.name,
            'url': self.url,
            'qa_data': self.qa_data,
            'summary': self.summary,
            'key_concepts': key_concepts_str,  # â­ ä½¿ç”¨è½¬æ¢åçš„å­—ç¬¦ä¸²åˆ—è¡¨
            'related_qas': self.related_qas
        }


class AgentMemory:
    """Agentè®°å¿†ï¼ˆåŸç‰ˆå®Œæ•´ä¿ç•™ï¼‰"""
    
    def __init__(self):
        self.qa = dict(question=None, answer=None)
        self.statements = []
        self.relevant = []
        self.edit_history = []
        self.qa_history = []
        self.uid = None
    
    def repr(self):
        # ========================================
        # ğŸ”§ ä¼˜åŒ–ï¼šæ˜¾ç¤ºæ›´å¤šç›¸å…³QAï¼ˆä¸åªæ˜¯å·²ç»„åˆçš„ï¼‰
        # é—®é¢˜ï¼šåŸæ¥åªæ˜¾ç¤ºå·²ç»„åˆçš„QAï¼ˆ1ä¸ªï¼‰ï¼ŒLLMåœ¨choose_actionæ—¶çœ‹ä¸åˆ°å…¶ä»–å€™é€‰
        # è§£å†³ï¼šåŒæ—¶æ˜¾ç¤ºå·²ç»„åˆçš„QA + å®ƒä»¬çš„ç›¸å…³QAï¼ˆå€™é€‰ï¼‰
        # ========================================
        relevant = '\n'.join([f'- [{e.name}] (ID: {e.id})' for e in self.relevant])
        
        # â­ æ–°å¢ï¼šæ˜¾ç¤ºå€™é€‰QAï¼ˆä»ç¬¬ä¸€ä¸ªå®ä½“çš„related_qasä¸­å–å‰5ä¸ªï¼‰
        candidates_str = ""
        if self.relevant and len(self.relevant) > 0 and hasattr(self.relevant[0], 'related_qas'):
            candidate_ids = self.relevant[0].related_qas[:5]  # å–å‰5ä¸ª
            if candidate_ids:
                candidates_str = f"\n\nå¯é€‰æ‹©çš„å€™é€‰QAï¼ˆæ¥è‡ª{self.relevant[0].name}ï¼‰ï¼š\n```txt\n"
                candidates_str += '\n'.join([f'- [QA-{cid}] (ID: {cid})' for cid in candidate_ids])
                candidates_str += "\n```"
        
        statements = '\n'.join(self.statements)
        return f"""
å½“å‰é—®é¢˜: {self.qa['question']}
å½“å‰ç­”æ¡ˆ: {self.qa['answer']}

ç›¸å…³æŠ€æœ¯é™ˆè¿°ï¼š
```txt
{statements}
```

ç›¸å…³QAå®ä½“åˆ—è¡¨ï¼ˆå·²ç»„åˆçš„æºQAï¼‰ï¼š
```txt
{relevant}
```{candidates_str}
"""
    
    def statements_repr(self, additional=None):
        return '\n'.join(self.statements + (additional or []))
    
    def dict(self):
        return {
            'qa': self.qa,
            'relevant': [e.dict() for e in self.relevant],
            'statements': self.statements,
            'edit_history': self.edit_history,
            'qa_history': self.qa_history,
            'uid': self.uid
        }
